@article{pfau2016connecting,
  title={Connecting generative adversarial networks and actor-critic methods},
  author={Pfau, David and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1610.01945},
  year={2016}
} %For relating actor-critic and GAN to bi-level

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
} %Original formulation of GAN

@inproceedings{chung2015recurrent,
  title={A recurrent latent variable model for sequential data},
  author={Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2980--2988},
  year={2015}
} %Recurrent latent variational model for prior on Theta beliefs

@article{metz2016unrolled,
  title={Unrolled generative adversarial networks},
  author={Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1611.02163},
  year={2016}
} %Trying to solve same thing with GAN's

@inproceedings{maclaurin2015autograd,
  title={Autograd: Effortless gradients in numpy},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
  booktitle={ICML 2015 AutoML Workshop},
  year={2015}
} %Autograd

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
} %Adam optimizer

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={2113--2122},
  year={2015}
} %Get response function for machine learning

@inproceedings{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
} %Bayesian optimization of hyperparameters

@book{kunapuli2008bilevel,
  title={A bilevel optimization approach to machine learning},
  author={Kunapuli, Gautam},
  year={2008},
  publisher={Rensselaer Polytechnic Institute}
} %Machine learning a bi-level optimization

@article{ha2016hypernetworks,
  title={HyperNetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
} %Hypernetwork paper

@inproceedings{pedregosa2016hyperparameter,
  title={Hyperparameter optimization with approximate gradient},
  author={Pedregosa, Fabian},
  booktitle={International Conference on Machine Learning},
  pages={737--746},
  year={2016}
} %Response function with 2nd order information

@inproceedings{luketina2016scalable,
  title={Scalable gradient-based tuning of continuous regularization hyperparameters},
  author={Luketina, Jelena and Berglund, Mathias and Greff, Klaus and Raiko, Tapani},
  booktitle={International Conference on Machine Learning},
  pages={2952--2960},
  year={2016}
} %Response function with 2nd order information again

@article{feng2017gradient,
  title={Gradient-based Regularization Parameter Selection for Problems with Non-smooth Penalty Functions},
  author={Feng, Jean and Simon, Noah},
  journal={arXiv preprint arXiv:1703.09813},
  year={2017}
} %Another response function with 2nd order information.

@article{fu2016drmad,
  title={DrMAD: distilling reverse-mode automatic differentiation for optimizing hyperparameters of deep neural networks},
  author={Fu, Jie and Luo, Hongyin and Feng, Jiashi and Low, Kian Hsiang and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:1601.00917},
  year={2016}
} %Approximate unrolled optimization.

@inproceedings{domke2012generic,
  title={Generic methods for optimization-based modeling},
  author={Domke, Justin},
  booktitle={Artificial Intelligence and Statistics},
  pages={318--326},
  year={2012}
} %Original unrolled gradient descent paper.

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
} %MNIST citation

@article{brock2017smash,
  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},
  author={Brock, Andrew and Lim, Theodore and Ritchie, JM and Weston, Nick},
  journal={arXiv preprint arXiv:1708.05344},
  year={2017}
} %Architecture search with hypernets

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Feb},
  pages={281--305},
  year={2012}
} %Random search for hyperparameters

@book{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
} %Bayesian methods on neural networks

@article{arlot2010survey,
  title={A survey of cross-validation procedures for model selection},
  author={Arlot, Sylvain and Celisse, Alain and others},
  journal={Statistics surveys},
  volume={4},
  pages={40--79},
  year={2010},
  publisher={The author, under a Creative Commons Attribution License}
} %Cross validation survey

@book{fudenberg1998theory,
  title={The theory of learning in games},
  author={Fudenberg, Drew and Levine, David K},
  volume={2},
  year={1998},
  publisher={MIT press}
} %Exploration of best-response

@inproceedings{bruckner2011stackelberg,
  title={Stackelberg games for adversarial prediction problems},
  author={Br{\"u}ckner, Michael and Scheffer, Tobias},
  booktitle={Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={547--555},
  year={2011},
  organization={ACM}
} % Stackelberg competition

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
} %ReLU activation.

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Springer}
} %SGD

@book{rasmussen2006gaussian,
  title={Gaussian processes for machine learning},
  author={Rasmussen, Carl Edward and Williams, Christopher KI},
  volume={1},
  year={2006},
  publisher={MIT press Cambridge}
} % Gaussian process

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}  % Weight regularization?

@article{finn2017model,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1703.03400},
  year={2017}
} % MAML Meta learning

@article{li2016hyperband,
	title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
	author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	journal={arXiv preprint arXiv:1603.06560},
	year={2016}
} %hyperband

@article{swersky2014freeze,
	title={Freeze-thaw Bayesian optimization},
	author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
	journal={arXiv preprint arXiv:1406.3896},
	year={2014}
} % Freeze-thaw? TODO: read

@incollection{huys2015reward,
  title={Reward-based learning, model-based and model-free},
  author={Huys, Quentin JM and Cruickshank, Anthony and Seri{\`e}s, Peggy},
  booktitle={Encyclopedia of Computational Neuroscience},
  pages={2634--2641},
  year={2015},
  publisher={Springer}
} % Model based vs model free

@InProceedings{franceschi2017forward,
  title = 	 {Forward and Reverse Gradient-Based Hyperparameter Optimization},
  author = 	 {Luca Franceschi and Michele Donini and Paolo Frasconi and Massimiliano Pontil},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1165--1173},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/franceschi17a/franceschi17a.pdf},
}

@article{bahdanau2016actor,
  title={An actor-critic algorithm for sequence prediction},
  author={Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1607.07086},
  year={2016}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{nash1950equilibrium,
  title={Equilibrium Points in N-Person Games.},
  author={Nash, JF},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  volume={36},
  number={1},
  pages={48--49},
  year={1950}
}

@article{blundell2015weight,
  title={Weight uncertainty in neural networks},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  journal={arXiv preprint arXiv:1505.05424},
  year={2015}
} % Bayes by backprop

@book{lizotte2008practical,
  title={Practical bayesian optimization},
  author={Lizotte, Daniel James},
  year={2008},
  publisher={University of Alberta}
}
